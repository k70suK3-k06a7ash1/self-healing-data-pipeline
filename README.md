# self-healing-data-pipeline
概要: 既存のデータソース（CSV、JSON、APIなど）を読み込ませると、LLMがそのスキーマと内容を分析し、自動的に知識グラフにデータカタログ（メタデータ）を構築。必要に応じてPGLite/DuckDBへのETL（Extract, Transform, Load）処理を自動生成・実行し、Redwood SDKでその進捗と結果を管理・可視化する。

技術スタックの連携:

PGLite/DuckDB: 変換されたデータの格納先、一時的なステージングエリア。
Knowledge Graph: データソースのメタデータ（列名、データ型、意味論、関連するビジネスルールなど）、ETLジョブの定義、データリネージなどを格納。
localLLM:
データソースのプロファイリング（例: 列の意味推論、異常値検出）。
知識グラフへのメタデータ登録の支援。
ETL変換ロジック（SQL、Pythonスクリプトなど）の生成。
データの品質チェック、異常レポートの生成。
Redwood SDK:
データソースのアップロード・接続インターフェース。
構築されたデータカタログの閲覧・検索UI。
ETLジョブの実行状況モニタリング、ログ表示。
データ品質レポートの表示。
ユースケース:

新しいデータセットが手元に来た際に、手動でカタログ化する手間を省く。
データ間の関係性を自動で発見し、データ連携を容易にする。
データガバナンスとリネージ管理を支援する。